# -*- coding: utf-8 -*-
"""Iris_Model_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12RCpCfQKC8ivmUMzt6DnncxX4lZ8GUCn
"""

import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import KFold
from sklearn.pipeline import Pipeline

#Initialize 
seed= 7
np.random.seed(seed)

#load data 
import io
from google.colab import files
uploaded = files.upload()
df = pd.read_csv(io.StringIO(uploaded['iris.csv'].decode('utf-8')) , header = None)
df

dataset = df.values
x= dataset[:,0:4].astype(float)
y = dataset[:,4]

#encode class values as integers
encoder = LabelEncoder()
encoder.fit(y)
encoded_y = encoder.transform(y)

#Convert integers to dummy variables (i.e one hot encoding)
dummy_y = np_utils.to_categorical(encoded_y)

#define baseline model
def baseline_model():
  model = Sequential()
  model.add(Dense(8 , activation='relu' , input_shape=(4,)))
  model.add(Dense(3, activation = 'softmax' ))
  model.compile(optimizer = 'Adam' , loss='categorical_crossentropy' , metrics=['acc'])
  return model

#Run the model
estimator= KerasClassifier(build_fn = baseline_model, epochs= 200, batch_size=5 , verbose=1)

#kfold 
kfold= KFold(n_splits= 10, shuffle=True , random_state=seed)

#EVALUATE 
results= cross_val_score(estimator, x , dummy_y , cv=kfold)

print("Accuracy: %2f%%(%2f%%)"%(results.mean()*100,results.std()*100))

#to build smaller network

def smaller_model():
  smaller_model=Sequential()
  smaller_model.add(Dense(4, activation='relu' , input_shape=(4,)))
  smaller_model.add(Dense(3, activation='softmax'))
  smaller_model.compile(optimizer='Adam' , loss='categorical_crossentropy' , metrics=['accuracy'])
  return smaller_model

#Run 
estimator= KerasClassifier(build_fn= smaller_model , epochs= 100 , batch_size= 5 , verbose=0)
kfold = KFold(n_splits=10 , shuffle= True , random_state= seed)
results= cross_val_score(estimator , x , dummy_y , cv= kfold)
print('smaller_model: %2f%%(%2f%%)' %(results.mean()*100 , results.std()*100))

#Larger_model
def larger_model():
  larger_model= Sequential()
  larger_model.add(Dense(16 , activation='relu' , input_shape=(4,)))
  larger_model.add(Dense(8 , activation='relu'))
  larger_model.add(Dense(3 , activation='softmax'))
  larger_model.compile(optimizer = 'rmsprop' , loss= 'categorical_crossentropy' , metrics=['accuracy'])
  return larger_model

#RUN
estimator=KerasClassifier(build_fn= larger_model , epochs=100 , batch_size=5 , verbose=0)
kfold=KFold(n_splits=10 , shuffle=True , random_state=seed)
results= cross_val_score(estimator , x , dummy_y , cv=kfold)
print('larger_model :  %2f%%(%2f%%)' %(results.mean()*100 , results.std()*100))

#Model that ovefits
def overfit_model():
  overfit_model= Sequential()
  overfit_model.add(Dense(12 , activation='relu' , input_shape=(4,)))
  overfit_model.add(Dense(10 , activation='relu'))
  overfit_model.add(Dense(8 , activation='relu'))
  overfit_model.add(Dense(3 , activation='softmax'))
  overfit_model.compile(optimizer='rmsprop' , loss='categorical_crossentropy' ,metrics=['acc'])
  return overfit_model

estimator = KerasClassifier(build_fn = overfit_model , epochs= 200 , batch_size= 5 , verbose=0)
kfold= KFold(n_splits=10 , shuffle=True , random_state= seed)
results= cross_val_score(estimator , x , dummy_y , cv=kfold)
print('Overfit_Model : %2f%%(%2f%%)' %(results.mean()*100 , results.std()*100))

# Using functional API 
from keras.layers import Input , Dense
from keras.models import Model
def fun_model():
  a = Input(shape=(4,))
  z= Dense(8 , activation='relu')(a)
  b = Dense(3, activation='softmax')(z)
  model = Model(a,b)
  model.compile(optimizer='Adam' , loss= 'categorical_crossentropy' , metrics=['acc'])
  return model

estimator = KerasClassifier(build_fn = fun_model , epochs = 200 , batch_size = 5 , verbose=0)
kfold = KFold(n_splits = 10 , shuffle= True , random_state= seed)
results= cross_val_score(estimator , x , dummy_y , cv= kfold)
print('Accuracy: %2f%%(%2f%%)' %(results.mean()*100 , results.std()*100))

#Model Subclassing
import tensorflow as tf
class SubclassModel (tf.keras.Model):
  def __init__(self):
    super(SubclassModel , self).__init__()
    self.dense1 = tf.keras.layers.Dense(8 , activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(6 , activation= tf.nn.relu)
    self.dense3 = tf.keras.layers.Dense(3 , activation= tf.nn.softmax)
    
  def call(self, a):
    x= self.dense1(a)
    x= self.dense2(x)
    return self.dense3(x)
def lastModel():
  model= SubclassModel()
  model.compile(optimizer='Adam' , loss= 'categorical_crossentropy' , metrics=['acc'])
  return model

estimator= KerasClassifier(build_fn= lastModel , epochs= 200 , batch_size= 5 , verbose=0)
kfold= KFold(n_splits=10 , shuffle=True , random_state= seed)
results = cross_val_score(estimator , x , dummy_y , cv=kfold)
print('Accuracy: %2f%%(%2f%%)' %(results.mean()*100 , results.std()*100))

#build model from scratch
complete_data= dataset.copy()
np.random.shuffle(complete_data)
data = complete_data[:,0:4].astype('float32')
labels= complete_data[:,4]
le= LabelEncoder()
le.fit(labels)
encode_labels= le.transform(labels).astype('float32')
one_hot_encode_labels= np_utils.to_categorical(encode_labels)
print(one_hot_encode_labels)
print(data.dtype)
train_dataset = data[:100]
test_dataset = data[100:]
train_label= one_hot_encode_labels[:100]
test_label = one_hot_encode_labels[100:]
print(train_label.shape)

def build_model():
  model= Sequential()
  model.add(Dense(4 , activation='relu' , input_shape=(4,)))
  model.add(Dense(3 , activation='softmax'))
  model.compile(optimizer='Adam' , loss='categorical_crossentropy' , metrics=['acc'])
  return model

#kfold
k=4
num_val_samples= len(train_dataset)//k
num_epochs= 200

for i in range(k):
  print('processing fold #' , i)
  val_data= train_dataset[i*num_val_samples: (i+1)*num_val_samples]
  val_labels= train_label[i* num_val_samples : (i+1) * num_val_samples]
  partial_train_data = np.concatenate([train_dataset[:i *num_val_samples], train_dataset[(i+1)*num_val_samples:]], axis=0)
  partial_train_labels= np.concatenate([train_label[:i*num_val_samples], train_label[(i+1)*num_val_samples:]])
  model = build_model()
  model.fit(partial_train_data , partial_train_labels, validation_data=(val_data, val_labels), epochs=num_epochs, batch_size=1 , verbose=0)
  loss, acc= model.evaluate(val_data , val_labels , verbose=0)
  print(acc)

def last_Model():
  model=Sequential()
  model.add(Dense(4 , activation='relu' , input_shape= (4,)))
  model.add(Dense(3, activation='softmax'))
  model.compile(optimizer='Adam' , loss= 'categorical_crossentropy' , metrics=['acc'])
  return model

model= last_Model()
model.fit(train_dataset , train_label, epochs= 100 , batch_size=5 , verbose=0)

history = model.evaluate(test_dataset , test_label)
history

